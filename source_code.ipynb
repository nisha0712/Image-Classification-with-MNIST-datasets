{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "souce_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM_FPoRyWRFu",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification with MNIST Dataset\n",
        "\n",
        "MNIST dataset, which stands for Modified National Institute of Standards and Technology database. It is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "\n",
        "The MNIST dataset is one of the most common datasets used for image classification and accessible from many different \n",
        "sources. In fact, even Tensorflow and Keras allow us to import and download the MNIST dataset directly from their API.\n",
        "\n",
        "# Convolutional Neural Network\n",
        "\n",
        "One of the most powerful supervised deep learning techniques is the Convolutional Neural NetworksCNNs are mainly used for image classification although you may find other application areas such as natural language processing.\n",
        "\n",
        "We are capable of using many different layers in a convolutional neural network. However, convolution, pooling, and fully connect layers are the most important ones.\n",
        "\n",
        "# Layers of CNN\n",
        "\n",
        "1.Convolutional Layers\n",
        "\n",
        "Convolutional layer is the very first layer where we extract features from the images in our datasets.\n",
        "\n",
        "2.Pooling Layer\n",
        "\n",
        "When constructing CNNs, it is common to insert pooling layers after each convolution layer to reduce the spatial size of the representation to reduce the parameter counts which reduces the computational complexity.\n",
        "\n",
        "3.A Set of Fully Connected Layers\n",
        "\n",
        "A fully connected network is our RegularNet where each parameter is linked to one another to determine the true relation and effect of each parameter on the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRUoUUW-WRFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the MNIST dataset\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8bMkPjMWRF5",
        "colab_type": "code",
        "colab": {},
        "outputId": "7e1f44d0-3305-4317-a883-30134fb6094e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image_index = 7777 # You may select anything up to 60,000\n",
        "print(y_train[image_index]) # The label is 8\n",
        "plt.imshow(x_train[image_index], cmap='Greys')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4248034278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FVZ2vEWWRGC",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c0e52e6-5b63-4c0c-b974-2c781edeb92d"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzorB3EjWRGH",
        "colab_type": "code",
        "colab": {},
        "outputId": "3aba4283-8a93-4a64-bc21-c072223e11cd"
      },
      "source": [
        "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# Making sure that the values are float so that we can get decimal points after division\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Number of images in x_train 60000\n",
            "Number of images in x_test 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOwL5FAjWRGO",
        "colab_type": "code",
        "colab": {},
        "outputId": "5d302df2-a25c-4a5d-c0c9-ac97ffceff44"
      },
      "source": [
        "# Importing the required Keras modules containing model and layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBc9pGO1Wrvr",
        "colab_type": "text"
      },
      "source": [
        "**Creating Model and Adding Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwZHZJq3WRGT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "95fd2e70-62ca-4280-80bf-cc111046b0d5"
      },
      "source": [
        "# Creating a Sequential Model and adding the layers\n",
        "model = Sequential()\n",
        "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
        "#applying maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=tf.nn.relu))\n",
        "model.add(Dropout(0,2))\n",
        "model.add(Dense(10, activation=tf.nn.softmax))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9a2c14ec4e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#applying maxpooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7GEZZmuWRGY",
        "colab_type": "code",
        "colab": {},
        "outputId": "e0df3ce0-27b6-4bb4-8896-e06bb68240ca"
      },
      "source": [
        "#Compiling and Fitting the Model\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train,y=y_train, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 84s 1ms/step - loss: 0.1723 - acc: 0.9480\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0615 - acc: 0.9814\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0375 - acc: 0.9883\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0241 - acc: 0.9923\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 83s 1ms/step - loss: 0.0185 - acc: 0.9939\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 83s 1ms/step - loss: 0.0139 - acc: 0.9955\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0124 - acc: 0.9956\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0099 - acc: 0.9968\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0085 - acc: 0.9974\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 86s 1ms/step - loss: 0.0081 - acc: 0.9972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4212435160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ccReVnmWRGd",
        "colab_type": "code",
        "colab": {},
        "outputId": "bb297737-c95a-48fa-8391-bf1fc58d89a8"
      },
      "source": [
        "#Evaluating the Model\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 411us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08492604368976936, 0.9833]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8KSbxDdWRGh",
        "colab_type": "code",
        "colab": {},
        "outputId": "93ccbd01-fc12-483c-8350-12d5c9a4b83b"
      },
      "source": [
        "image_index = 4444\n",
        "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
        "pred = model.predict(x_test[image_index].reshape(1, 28,28, 1))\n",
        "print(pred.argmax())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4DLwC6GWRGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accuracy=98.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozW8MkUDWRGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}